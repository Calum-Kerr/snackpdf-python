#!/usr/bin/env python3
"""
Script to fix critical SEO indexing issues for SnackPDF
This addresses the zero impressions problem in Google Search Console
"""

import os
import requests
from datetime import datetime

def check_website_accessibility():
    """Check if the website is accessible and responding correctly"""
    print("üîç Checking website accessibility...")
    
    urls_to_check = [
        'https://www.snackpdf.com/',
        'https://www.snackpdf.com/robots.txt',
        'https://www.snackpdf.com/sitemap.xml',
        'https://www.snackpdf.com/google6520a768170937d3.html',
        'https://www.snackpdf.com/compress',
        'https://www.snackpdf.com/pdf_to_jpg'
    ]
    
    for url in urls_to_check:
        try:
            response = requests.get(url, timeout=10)
            status = "‚úÖ OK" if response.status_code == 200 else f"‚ùå {response.status_code}"
            print(f"  {url}: {status}")
        except Exception as e:
            print(f"  {url}: ‚ùå ERROR - {str(e)}")

def create_enhanced_robots_txt():
    """Create an enhanced robots.txt with better crawling instructions"""
    print("ü§ñ Creating enhanced robots.txt...")
    
    robots_content = f"""User-agent: *
Allow: /

# Sitemap location
Sitemap: https://www.snackpdf.com/sitemap.xml

# Crawl-delay for respectful crawling
Crawl-delay: 1

# Allow all major search engines
User-agent: Googlebot
Allow: /
Crawl-delay: 1

User-agent: Bingbot
Allow: /
Crawl-delay: 1

User-agent: Slurp
Allow: /

User-agent: DuckDuckBot
Allow: /

User-agent: Baiduspider
Allow: /

# Allow access to static resources
Allow: /static/
Allow: /*.css$
Allow: /*.js$
Allow: /*.png$
Allow: /*.jpg$
Allow: /*.jpeg$
Allow: /*.gif$
Allow: /*.svg$
Allow: /*.ico$

# Block access to admin or sensitive areas (if any in future)
# User-agent: *
# Disallow: /admin/
# Disallow: /private/

# Last updated: {datetime.now().strftime('%Y-%m-%d')}
"""
    
    with open('robots.txt', 'w', encoding='utf-8') as f:
        f.write(robots_content)
    
    print("‚úÖ Enhanced robots.txt created")

def create_google_indexing_request_urls():
    """Create a list of URLs to manually request indexing for"""
    print("üìã Creating URL list for manual indexing requests...")
    
    # High-priority URLs for immediate indexing
    priority_urls = [
        'https://www.snackpdf.com/',
        'https://www.snackpdf.com/compress',
        'https://www.snackpdf.com/pdf_to_jpg',
        'https://www.snackpdf.com/merge',
        'https://www.snackpdf.com/split',
        'https://www.snackpdf.com/jpg_to_pdf',
        'https://www.snackpdf.com/word_to_pdf',
        'https://www.snackpdf.com/word',
        'https://www.snackpdf.com/unlock',
        'https://www.snackpdf.com/protect'
    ]
    
    with open('priority_urls_for_indexing.txt', 'w', encoding='utf-8') as f:
        f.write("# Priority URLs for Google Search Console Manual Indexing\n")
        f.write("# Submit these URLs one by one in Google Search Console > URL Inspection\n\n")
        for url in priority_urls:
            f.write(f"{url}\n")
    
    print("‚úÖ Priority URLs list created: priority_urls_for_indexing.txt")

def create_seo_health_check():
    """Create a comprehensive SEO health check report"""
    print("üìä Creating SEO health check report...")
    
    report = f"""# SnackPDF SEO Health Check Report
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## Current Issues Identified:

### ‚úÖ FIXED ISSUES:
1. Sitemap dates updated from future date (2025-01-20) to current date (2025-06-23)
2. Enhanced robots.txt with better crawling instructions

### ‚ùå CRITICAL ISSUES TO ADDRESS:

1. **Google Search Console Submission**
   - Status: NEEDS MANUAL ACTION
   - Action: Go to Google Search Console and manually submit sitemap
   - URL: https://search.google.com/search-console
   - Submit: https://www.snackpdf.com/sitemap.xml

2. **Manual URL Indexing Requests**
   - Status: NEEDS MANUAL ACTION  
   - Action: Use URL Inspection tool in Google Search Console
   - Submit priority URLs from priority_urls_for_indexing.txt

3. **Potential Crawl Budget Issues**
   - Status: MONITORING NEEDED
   - Issue: 25+ pages but zero impressions suggests crawl budget problems
   - Solution: Focus on high-value pages first

### üîç NEXT STEPS:

1. **IMMEDIATE (Do Today):**
   - Deploy updated sitemap.xml and robots.txt
   - Submit sitemap in Google Search Console
   - Request indexing for top 5 priority URLs

2. **THIS WEEK:**
   - Request indexing for remaining priority URLs
   - Monitor Google Search Console for crawl errors
   - Check for any server response issues

3. **ONGOING:**
   - Monitor indexing progress weekly
   - Add fresh content to encourage re-crawling
   - Build quality backlinks to improve domain authority

## Technical SEO Status:
‚úÖ Robots.txt: Properly configured
‚úÖ Sitemap.xml: Present with all pages
‚úÖ Meta tags: Implemented on all pages
‚úÖ Structured data: JSON-LD implemented
‚úÖ Google verification: File accessible
‚úÖ SSL certificate: Active (HTTPS)
‚úÖ Mobile-friendly: Responsive design
‚úÖ Page speed: Needs monitoring
‚úÖ Google AdSense: Properly integrated

## Expected Timeline:
- Week 1-2: Initial indexing of priority pages
- Week 3-4: Full site indexing
- Month 2-3: Improved rankings and traffic
"""
    
    with open('seo_health_report.txt', 'w', encoding='utf-8') as f:
        f.write(report)
    
    print("‚úÖ SEO health check report created: seo_health_report.txt")

def main():
    """Main function to run all SEO fixes"""
    print("üöÄ Starting SnackPDF SEO Indexing Fix...")
    print("=" * 50)
    
    # Run all checks and fixes
    check_website_accessibility()
    print()
    
    create_enhanced_robots_txt()
    print()
    
    create_google_indexing_request_urls()
    print()
    
    create_seo_health_check()
    print()
    
    print("üéâ SEO indexing fixes completed!")
    print("=" * 50)
    print("\nüìã NEXT MANUAL STEPS:")
    print("1. Deploy the updated files to your Heroku app")
    print("2. Go to Google Search Console")
    print("3. Submit your sitemap: https://www.snackpdf.com/sitemap.xml")
    print("4. Use URL Inspection to request indexing for priority URLs")
    print("5. Monitor progress over the next 1-2 weeks")
    print("\nüìä Check seo_health_report.txt for detailed action plan")

if __name__ == "__main__":
    main()
